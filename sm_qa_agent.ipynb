{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e53e8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from typing import TypedDict , Annotated\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de524b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path = \"sa.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d556f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-V3.2\", \n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.0,\n",
    "    streaming = True\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe181507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class qastate(TypedDict):\n",
    "    messages : Annotated[list[BaseMessage], add_messages]\n",
    "    state : str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25e9f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(state : qastate):\n",
    "    response = state[\"messages\"][-1].content\n",
    "    prompt = f\"Analyze the given content {response} \\n Answer only in 4 words, If you find the content clear respond with :'answer', \\n If you find the content Ambiguous respond with : 'clarify', \\n If you find the content too broad respond with :'narrow', \\n If you find the content that it is asking for the opinion respond with : 'opinion\"\n",
    "    st = model.invoke(prompt).content.lower().strip()\n",
    "\n",
    "    if \"answer\" in st:\n",
    "        st = \"answer\"\n",
    "    elif 'clarify' in st:\n",
    "        st = \"clarify\"\n",
    "    elif \"narrow\" in st:\n",
    "        st = \"narrow\"\n",
    "    elif \"opinion\" in st:\n",
    "        st = \"opinion\"\n",
    "    else:\n",
    "        st = \"answer\"  #Be safe\n",
    "\n",
    "    return {\"state\" : st}\n",
    "\n",
    "\n",
    "def answer(state : qastate):\n",
    "    \n",
    "    response = state[\"messages\"]\n",
    "    result = model.invoke(response).content\n",
    "\n",
    "    return {\"messages\" : [AIMessage(content = result)]}  \n",
    "\n",
    "def ambiguous(state : qastate):\n",
    "    response = state[\"messages\"]\n",
    "    prompt = f\"You find the content {response} ambiguous, Ask the user for the clarity in the content\"\n",
    "    am = model.invoke(prompt).content\n",
    "\n",
    "    return {\"messages\" : [AIMessage(content = am)]}\n",
    "\n",
    "def broad(state : qastate):\n",
    "    response = state[\"messages\"]\n",
    "    prompt = f\"You find the content {response} broad, Ask the user to narrow down the content.\"\n",
    "    br = model.invoke(prompt).content\n",
    "\n",
    "    return {\"messages\" : [AIMessage(content = br)]}\n",
    "\n",
    "def opinion(state : qastate):\n",
    "    response = state[\"messages\"]\n",
    "    prompt = f\"You find the content {response} as opinion base, so you share your opinion as per your knowledge.\"\n",
    "    pr = model.invoke(prompt).content\n",
    "\n",
    "    return {\"messages\" : [AIMessage(content = pr)]}\n",
    "\n",
    "def condition(state : qastate):\n",
    "    if state[\"state\"] == \"answer\":\n",
    "        return \"answer\"\n",
    "    elif state[\"state\"] == \"clarify\":\n",
    "        return \"ambiguous\"\n",
    "    elif state[\"state\"] == \"narrow\":\n",
    "        return \"broad\"\n",
    "    elif state[\"state\"] == \"opinion\":\n",
    "        return \"opinion\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6fcb517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(qastate)\n",
    "\n",
    "graph.add_node(\"analyze\", analyze)\n",
    "graph.add_node(\"answer\", answer)\n",
    "graph.add_node(\"ambiguous\", ambiguous)\n",
    "graph.add_node(\"broad\", broad)\n",
    "graph.add_node(\"opinion\", opinion)\n",
    "\n",
    "graph.add_edge(START, \"analyze\")\n",
    "graph.add_conditional_edges(\"analyze\",condition)\n",
    "graph.add_edge(\"answer\", END)\n",
    "graph.add_edge(\"ambiguous\", END)\n",
    "graph.add_edge(\"broad\", END)\n",
    "graph.add_edge(\"opinion\", END)\n",
    "\n",
    "\n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "80edb91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you please clarify what aspect of Tesla you're interested in? Are you asking about their current electric vehicle lineup, their automotive technology, their history and mission, or something else related to the company?\n"
     ]
    }
   ],
   "source": [
    "result = workflow.invoke({\"messages\" : [HumanMessage(content = \"Tell me about Tesla?\")]})\n",
    "\n",
    "ai_response  = result[\"messages\"][-1].content\n",
    "\n",
    "print(ai_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
